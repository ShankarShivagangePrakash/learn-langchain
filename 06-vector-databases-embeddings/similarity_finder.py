# =============================================================================
# TEXT SIMILARITY FINDER USING VECTOR EMBEDDINGS
# =============================================================================
"""
Educational Demo: Text Similarity Analysis Using OpenAI Embeddings

This script demonstrates how to calculate semantic similarity between two text 
inputs using vector embeddings. It showcases the practical application of 
embeddings in natural language processing for measuring text relatedness.

TECHNICAL CONCEPTS COVERED:
‚Ä¢ OpenAI text-embedding-ada-002 model usage
‚Ä¢ Vector embeddings generation (1536 dimensions)
‚Ä¢ Dot product similarity calculation
‚Ä¢ Semantic similarity vs. lexical similarity
‚Ä¢ High-dimensional vector mathematics

EDUCATIONAL VALUE:
‚Ä¢ Understand how AI models represent text as numerical vectors
‚Ä¢ Learn practical similarity measurement techniques
‚Ä¢ Explore real-world applications of embedding technology
‚Ä¢ Compare semantic understanding vs. keyword matching

BUSINESS APPLICATIONS:
‚Ä¢ Document similarity analysis
‚Ä¢ Content recommendation systems
‚Ä¢ Duplicate detection algorithms
‚Ä¢ Search relevance scoring
‚Ä¢ Customer query matching
"""

import os
from langchain_openai import OpenAIEmbeddings
import numpy as np

# =============================================================================
# ENVIRONMENT CONFIGURATION & MODEL INITIALIZATION
# =============================================================================

# Retrieve OpenAI API key from environment variables
# Security Best Practice: Never hardcode API keys in source code
# Set environment variable: export OPENAI_API_KEY="your-api-key-here"
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Initialize OpenAI Embeddings model
# Technical Details:
# - Model: text-embedding-ada-002 (OpenAI's most capable embedding model)
# - Output Dimensions: 1536 (high-dimensional vector space)
# - Token Limit: ~8,191 tokens per input
# - Cost: $0.0001 per 1K tokens (as of 2024)
# - Use Cases: Semantic search, similarity, clustering, classification
embeddings_model = OpenAIEmbeddings(api_key=OPENAI_API_KEY)

# =============================================================================
# USER INPUT COLLECTION & VALIDATION
# =============================================================================

print("üîç Text Similarity Finder Using Vector Embeddings")
print("=" * 50)
print("Compare the semantic similarity between two pieces of text.")
print("Examples to try:")
print("‚Ä¢ 'I love programming' vs 'I enjoy coding'")
print("‚Ä¢ 'The weather is sunny' vs 'It's a bright day'")
print("‚Ä¢ 'Machine learning' vs 'Artificial intelligence'")
print("‚Ä¢ 'Dog' vs 'Cat' (for contrast)")
print()

# Collect first text input from user
# Educational Note: The model can handle various text lengths and formats
# - Single words, phrases, sentences, or paragraphs
# - Technical terms, colloquial language, or formal text
# - Multi-language support (though performance varies by language)
text1 = input("üìù Enter the first text: ").strip()

# Collect second text input for comparison
text2 = input("üìù Enter the second text: ").strip()

# Input validation
if not text1 or not text2:
    print("‚ùå Error: Both text inputs are required.")
    print("üí° Please provide two non-empty text strings for comparison.")
    exit(1)

print(f"\nüîÑ Analyzing similarity between:")
print(f"   Text 1: '{text1}'")
print(f"   Text 2: '{text2}'")

# =============================================================================
# VECTOR EMBEDDING GENERATION
# =============================================================================

print(f"\n‚ö° Generating embeddings...")
print(f"üìä Converting text to 1536-dimensional vectors...")

# Generate embedding vector for first text
# Technical Process:
# 1. Text tokenization (breaking into subword tokens)
# 2. Token encoding (converting to numerical IDs)
# 3. Model processing through transformer architecture
# 4. Output: Dense vector representation capturing semantic meaning
# 5. Vector normalization for consistent similarity calculations
response1 = embeddings_model.embed_query(text1)

# Generate embedding vector for second text
# Note: Each embed_query() call makes an API request to OpenAI
# Cost Consideration: Each call consumes tokens based on input length
# Performance: Typical response time 100-500ms depending on text length
response2 = embeddings_model.embed_query(text2)

# =============================================================================
# SIMILARITY CALCULATION & ANALYSIS
# =============================================================================

print(f"‚úÖ Embeddings generated successfully!")
print(f"üìê Calculating semantic similarity...")

# Calculate dot product similarity score
# Mathematical Explanation:
# - Dot product: sum of element-wise multiplication of two vectors
# - Formula: similarity = Œ£(a_i √ó b_i) for i = 1 to n
# - Higher values indicate greater similarity
# - Range: Typically -1 to 1 for normalized vectors
# - OpenAI embeddings are normalized, so range is roughly 0.0 to 1.0
similarity_score = np.dot(response1, response2)

# =============================================================================
# RESULTS DISPLAY & INTERPRETATION
# =============================================================================

# Convert similarity score to percentage for user-friendly display
# Technical Note: Multiplying by 100 for percentage representation
# The original similarity score (0.0-1.0) becomes (0%-100%)
similarity_percentage = similarity_score * 100

print(f"\nüéØ **SIMILARITY ANALYSIS RESULTS**")
print(f"=" * 40)
print(f"üìä Similarity Score: {similarity_score:.4f}")
print(f"üìà Similarity Percentage: {similarity_percentage:.2f}%")

# Provide interpretation of the similarity score
# Educational Guidance: Help users understand what the numbers mean
if similarity_percentage >= 90:
    interpretation = "üü¢ EXTREMELY SIMILAR - Nearly identical meaning"
    explanation = "These texts express very similar or identical concepts."
elif similarity_percentage >= 75:
    interpretation = "üîµ HIGHLY SIMILAR - Strong semantic relationship"
    explanation = "These texts are closely related in meaning and context."
elif similarity_percentage >= 60:
    interpretation = "üü° MODERATELY SIMILAR - Some semantic overlap"
    explanation = "These texts share some common themes or concepts."
elif similarity_percentage >= 40:
    interpretation = "üü† SOMEWHAT SIMILAR - Limited semantic connection"
    explanation = "These texts have minor similarities but different overall meanings."
else:
    interpretation = "üî¥ DISSIMILAR - Little to no semantic relationship"
    explanation = "These texts express different concepts with minimal overlap."

print(f"\n{interpretation}")
print(f"üí° {explanation}")

# =============================================================================
# EDUCATIONAL INSIGHTS & TECHNICAL DETAILS
# =============================================================================

print(f"\nüìö **UNDERSTANDING THE RESULTS:**")
print(f"")
print(f"üî¨ **How Similarity Was Calculated:**")
print(f"   1. Each text converted to 1536-dimensional vector")
print(f"   2. Vectors capture semantic meaning, not just keywords")
print(f"   3. Dot product calculated between the two vectors")
print(f"   4. Higher scores = more similar meanings")
print(f"")
print(f"üß† **Why This Works:**")
print(f"   ‚Ä¢ Embeddings capture context and meaning")
print(f"   ‚Ä¢ Similar concepts cluster together in vector space")
print(f"   ‚Ä¢ Mathematical operations reveal semantic relationships")
print(f"   ‚Ä¢ Model trained on vast text data learns word associations")

print(f"\nüîç **COMPARISON WITH OTHER METHODS:**")
print(f"")
print(f"üìù Keyword Matching:")
print(f"   ‚Ä¢ Only finds exact word matches")
print(f"   ‚Ä¢ Misses synonyms and related concepts")
print(f"   ‚Ä¢ Example: 'car' vs 'automobile' = 0% similarity")
print(f"")
print(f"ü§ñ Semantic Embeddings (This Method):")
print(f"   ‚Ä¢ Understands meaning and context")
print(f"   ‚Ä¢ Recognizes synonyms and related terms")
print(f"   ‚Ä¢ Example: 'car' vs 'automobile' = ~85% similarity")

# Display technical details about the embedding vectors
print(f"\n‚öôÔ∏è **TECHNICAL DETAILS:**")
print(f"   ‚Ä¢ Embedding Model: text-embedding-ada-002")
print(f"   ‚Ä¢ Vector Dimensions: {len(response1)}")
print(f"   ‚Ä¢ Vector Length (Text 1): {np.linalg.norm(response1):.4f}")
print(f"   ‚Ä¢ Vector Length (Text 2): {np.linalg.norm(response2):.4f}")
print(f"   ‚Ä¢ Similarity Metric: Dot Product")
print(f"   ‚Ä¢ Score Range: 0.0 (dissimilar) to 1.0 (identical)")

# =============================================================================
# PRACTICAL APPLICATIONS & USE CASES
# =============================================================================

print(f"\nüöÄ **REAL-WORLD APPLICATIONS:**")
print(f"""
üè¢ BUSINESS USE CASES:
‚Ä¢ Document similarity analysis for knowledge management
‚Ä¢ Customer support query routing and matching
‚Ä¢ Content recommendation systems (articles, products)
‚Ä¢ Duplicate content detection and deduplication
‚Ä¢ Search result relevance scoring and ranking

üî¨ RESEARCH & ACADEMIC:
‚Ä¢ Literature review and paper similarity analysis
‚Ä¢ Plagiarism detection and academic integrity
‚Ä¢ Research topic clustering and categorization
‚Ä¢ Citation analysis and reference matching

üíª DEVELOPMENT & TECHNICAL:
‚Ä¢ Code similarity detection and refactoring
‚Ä¢ API documentation matching and discovery
‚Ä¢ Bug report similarity for issue tracking
‚Ä¢ Feature request categorization and prioritization

üìä DATA SCIENCE & ANALYTICS:
‚Ä¢ Customer feedback sentiment and topic analysis
‚Ä¢ Social media content categorization
‚Ä¢ Survey response similarity and clustering
‚Ä¢ Market research and competitive analysis
""")

print(f"\n‚ú® **TRY MORE COMPARISONS:**")
print(f"   ‚Ä¢ Technical terms vs. plain language explanations")
print(f"   ‚Ä¢ Different languages expressing same concepts")
print(f"   ‚Ä¢ Formal vs. informal expressions of ideas")
print(f"   ‚Ä¢ Product descriptions vs. customer reviews")
print(f"   ‚Ä¢ Questions vs. potential answers")

print(f"\nüéØ **TIPS FOR BETTER RESULTS:**")
print(f"   ‚Ä¢ Use complete sentences for better context")
print(f"   ‚Ä¢ Include relevant keywords and concepts")
print(f"   ‚Ä¢ Consider the domain and context of your text")
print(f"   ‚Ä¢ Longer texts generally provide more context")
print(f"   ‚Ä¢ Be specific about the concepts you're comparing")